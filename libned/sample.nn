fn range(int n):
    array<int, n> result
    int i = 0
    while i < n:
        result[i] = i
        i += 1
    return result

fn zip<type T, type U, int N>(array<T, N> arr1, array<T, N> arr2):
    array<tuple<T, U>, N> result
    for int i in range(N):
        result[i][0] = arr1[i]
        result[i][1] = arr2[i]
    return result

fn enumerate<type T, int N>(array<T, N> data):
    return zip(range(N), data)

struct node<type T>:
    T val
    bool has_val
    node<T> next

struct list<type T>:
    node<T> root
    node<T> end
    int size

fn create<type T>():
    list<T> list
    list.end = list.root
    list.size = 0
    return list

fn append<type T>(list<T> self, T val):
    self.end.has_val = true
    self.end.val = val
    node<T> new_end
    self.end.next = new_end
    self.end = new_end
    self.size += 1

fn to_array<type T>(list<T> self):
    array<T, self.size> result
    node<T> nd = self.root
    for i in range(nd.size):
        result[i] = nd.val
        nd = nd.next
    return result

fn format(string fmt, string *params):
    raise "Not implemented"

intr add_same_intr<fptype fout, fptype f1, fptype f2, int *shape>(tensor<f1, *shape> left, tensor<f2, *shape> right):
    tensor<fout, *shape> out
    return out

intr const<float val>():
    tensor<1> out
    return out


intr reshape<int *out_shape, <int *inp_shape>>(tensor<*inp_shape> inp):
    int inp_size = 1
    for int dim_size in inp_shape:
        inp_size *= dim_size

    int out_size = 1
    int unknown_idx = -1
    for int i in range(out_shape.length):
        if out_shape[i] == -1:
            if unknown_idx != -1:
                raise "Multiple -1 values in output shape"
            unknown_idx = i
        else:
            out_size *= out_shape[i]

    if unknown_idx != -1:
        out_shape[unknown_idx] = inp_size / out_size
        out_size *= out_shape[unknown_idx]

    if out_size != inp_size:
        raise format("Unable to transform tensor with size {} to size {}", inp_size, out_size)
    tensor<*out_shape> out
    return out


intr concat<int dim, <int *t1_shape>, <int *t2_shape>>(tensor<*t1_shape> t1, tensor<*t2_shape> t2):
    if t1_shape.length != t2_shape.length:
        raise "Mismatching tensor ranks"
    if t1_shape.length <= dim:
        raise "Invalid concatenation dimension"
    
    for int i in range(t1_shape.length):
        if i != dim and t1_shape[i] != t2_shape[i]:
            raise "Mismatching tensor shapes"

    t1_shape[dim] += t2_shape[dim]
    tensor<*t1_shape> out
    return out


intr relu<int *shape>(tensor<*shape> t1):
    tensor<*shape> out
    return out


intr softmax<int *shape>(tensor<*shape> t1):
    tensor<*shape> out
    return out


intr matmul<int N, int M, int S>(tensor<N, S> t1, tensor<S, M> t2):
    tensor<N, M> out
    return out


intr conv2d_intr<int C, int H, int W, int K, int M, int N>(tensor<C, H, W> inp, tensor<K, C, M, N> kernel):
    tensor<K, H, W> out
    return out


def xavier<int *shape>():
    tensor<*shape> param
    return param


def linear<int N, int M>(tensor<N> inp):
    return matmul(xavier<M, N>(), reshape<N, 1>(inp)) + xavier<N>()


def conv2d<int K, int M, int N, int C, int H, int W>(tensor<C, H, W> inp):
    return conv2d_intr(inp, xavier<K, C, M, N>())


def rnn_cell<int I, int S>(tensor<I> inp, tensor<S> state):
    tensor combined = concat<1>(reshape<N, -1>(inp), reshape<N, -1>(state))
    tensor layer1 = relu(linear(combined))
    tensor output = softmax(linear(layer1))
    tensor nstate = relu(linear(layer1))
    return nstate, output


def rnn<int I, int S>(tensor<I> inp):
    static tensor<S> state
    
    tuple result = rnn_cell<I, S>(inp, state)
    state = result[0]
    return result[1]


def model<int G>(tensor<3, 312, 312> img):
    tensor layer1 = relu(conv2d<8, 3, 3>(img))
    tensor layer2 = relu(conv2d<16, 3, 3>(layer1))

    tensor<16, 312, 312> tmp = layer2
    for int i in range(G):
        tmp = relu(conv2d<16, 3, 3>(tmp))
    
    tensor flattened = reshape<N, -1>(tmp);
    tensor output = rnn<N, flattened.shape[-1], 128>(flattened)

    return output
