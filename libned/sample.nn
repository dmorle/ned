fn range(int n):
    array<int, n> result
    int i = 0
    while i < n:
        result[i] = i
        i += 1
    return result


fn zip(array head, array* tail):
    for array arr in tail:
        if head.length != arr.length:
            raise "Array length mismatch for zip"
    array<type, tail.length + 1> dtype_data
    dtype_data[0] = head.dtype
    for int i in range(tail.length):
        dtype_data[i + 1] = tail[i].dtype
    
    array<dtype_data, head.length> result;
    for int i in range(head.length):
        result[i][0] = head[i]
        for int j in range(tail.length):
            result[i][j + 1] = tail[j][i]
    return result


fn enumerate(array data):
    array<tuple<int, data.dtype>, data.length> result
    for int i in range(data.length):
        result[i][0] = i
        result[i][1] = data[i]
    return result


fn format(string fmt, string *params):
    raise "Not implemented"


intr const<float val>():
    tensor<1> out
    return out


intr reshape<int *out_shape, <int *inp_shape>>(tensor<*inp_shape> inp):
    int inp_size = 1
    for int dim_size in inp_shape:
        inp_size *= dim_size

    int out_size = 1
    int unknown_idx = -1
    for int i in range(out_shape.length):
        if out_shape[i] == -1:
            if unknown_idx != -1:
                raise "Multiple -1 values in output shape"
            unknown_idx = i
        else:
            out_size *= out_shape[i]

    if unknown_idx != -1:
        out_shape[unknown_idx] = inp_size / out_size
        out_size *= out_shape[unknown_idx]

    if out_size != inp_size:
        raise format("Unable to transform tensor with size {} to size {}", inp_size, out_size)
    tensor<*out_shape> out
    return out


intr concat<int dim, <int *t1_shape>, <int *t2_shape>>(tensor<*t1_shape> t1, tensor<*t2_shape> t2):
    if t1_shape.length != t2_shape.length:
        raise "Mismatching tensor ranks"
    if t1_shape.length <= dim:
        raise "Invalid concatenation dimension"
    
    for int i in range(t1_shape.length):
        if i != dim and t1_shape[i] != t2_shape[i]:
            raise "Mismatching tensor shapes"

    t1_shape[dim] += t2_shape[dim]
    tensor<*t1_shape> out
    return out


intr relu<int *shape>(tensor<*shape> t1):
    tensor<*shape> out
    return out


intr softmax<int *shape>(tensor<*shape> t1):
    tensor<*shape> out
    return out


intr matmul<int N, int M, int S>(tensor<N, S> t1, tensor<S, M> t2):
    tensor<N, M> out
    return out


intr conv2d_intr<int C, int H, int W, int K, int M, int N>(tensor<C, H, W> inp, tensor<K, C, M, N> kernel):
    tensor<K, H, W> out
    return out


def xavier<int *shape>():
    tensor<*shape> param
    return param


def linear<int N, int M>(tensor<N> inp):
    return matmul(xavier<M, N>(), reshape<N, 1>(inp)) + xavier<N>()


def conv2d<int K, int M, int N, int C, int H, int W>(tensor<C, H, W> inp):
    return conv2d_intr(inp, xavier<K, C, M, N>())


def rnn_cell<int I, int S>(tensor<I> inp, tensor<S> state):
    tensor combined = concat<1>(reshape<N, -1>(inp), reshape<N, -1>(state))
    tensor layer1 = relu(linear(combined))
    tensor output = softmax(linear(layer1))
    tensor nstate = relu(linear(layer1))
    return nstate, output


def rnn<int I, int S>(tensor<I> inp):
    static tensor<S> state
    
    tuple result = rnn_cell<I, S>(inp, state)
    state = result[0]
    return result[1]


def model<int G>(tensor<3, 312, 312> img):
    tensor layer1 = relu(conv2d<8, 3, 3>(img))
    tensor layer2 = relu(conv2d<16, 3, 3>(layer1))

    tensor<16, 312, 312> tmp = layer2
    for int i in range(G):
        tmp = relu(conv2d<16, 3, 3>(tmp))
    
    tensor flattened = reshape<N, -1>(tmp);
    tensor output = rnn<N, flattened.shape[-1], 128>(flattened)

    return output
