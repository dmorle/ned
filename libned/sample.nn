fn min<type T>(T left, T right) -> T:
    if left < right:
        return left
    return right

fn max<type T>(T left, T right) -> T:
    if left > right:
        return left
    return right

fn range(int n) -> array<int>:
    array<int> result
    int i = 0
    while i < n:
        result += [i]
        i += 1
    return result

fn map<type T, type U>(array<T> data, fn f(T) -> U) -> array<U>:
    array<U> out
    for var e in data:
        out += [f(e)]
    return out

fn zip<type *T>(array<T> *arrs) -> array<tuple<*T>>:
    int min_len = 0
    for var i in range(len(arrs)):
        min_len = min(min_len, len(arrs[i]))

    array<tuple<*T>> result
    for var i in range(min_len):
        tuple<*T> elem
        for var j in range(len(arrs)):
            elem[j] = arrs[j][i]
        result += [elem]
    return result

fn enumerate<type T>(array<T> data) -> array<tuple<int, T>>:
    return zip(range(len(data)), data)

struct node<type T>:
    T val
    bool has_val
    node<T> next

struct list<type T>:
    node<T> root
    int size

fn create<type T>() -> list<T>:
    list<T> arr
    node<T> head
    arr.root = head
    arr.size = 0
    return arr

fn append<type T>(list<T> self, T val):
    self.end.has_val = true
    self.end.val = val
    node<T> new_end
    self.end.next = new_end
    self.end = new_end
    self.size += 1

fn to_array<type T>(list<T> self) -> array<T>:
    array<T, self.size> result
    node<T> nd = self.root
    for i in range(nd.size):
        result[i] = nd.val
        nd = nd.next
    return result

fn format(string fmt, string *params):
    raise "Not implemented"

intr add_same_intr<fp fout, fp *fin, int *shape>(fin<*shape> *inp) -> out:
    return fout<*shape>

intr add_same_intr<fp fout, fp f1, fp f2, int *shape>(f1<*shape> left, f2<*shape> right) -> out:
    return fout<*shape>

intr const_intr<fp fout, float val>() -> out:
    return fout<>

intr reshape<int *out_shape, fp fty, int *inp_shape>(fty<*inp_shape> inp) -> out:
    var inp_size = 1
    for var dim_size in inp_shape:
        inp_size *= dim_size

    int out_size = 1
    int unknown_idx = -1
    for int i in range(out_shape.length):
        if out_shape[i] == -1:
            if unknown_idx != -1:
                raise "Multiple -1 values in output shape"
            unknown_idx = i
        else:
            out_size *= out_shape[i]

    if unknown_idx != -1:
        out_shape[unknown_idx] = inp_size / out_size
        out_size *= out_shape[unknown_idx]

    if out_size != inp_size:
        raise format("Unable to transform tensor with size {} to size {}", inp_size, out_size)
    fp<fty> out
    return out

intr concat<int dim, fp fty, int *t1_shape, int *t2_shape>(fty<*t1_shape> t1, fty<*t2_shape> t2) -> out:
    if len(t1_shape) != len(t2_shape):
        raise "Mismatching tensor ranks"
    if len(t1_shape) <= dim:
        raise "Invalid concatenation dimension"
    
    for int dim1, int dim2 in range(len(t1_shape)):
        if i != dim and t1_shape[i] != t2_shape[i]:
            raise "Mismatching tensor shapes"

    t1_shape[dim] += t2_shape[dim]
    fty<*t1_shape> out
    return out

intr relu<fp fty, int *shape>(fty<*shape> t1) -> out:
    fty<*shape> out
    return out

intr softmax<int *shape>(tensor<*shape> t1):
    tensor<*shape> out
    return out

intr matmul<int N, int M, int S>(tensor<N, S> t1, tensor<S, M> t2):
    tensor<N, M> out
    return out

intr conv2d_intr<int C, int H, int W, int K, int M, int N>(tensor<C, H, W> inp, tensor<K, C, M, N> kernel):
    tensor<K, H, W> out
    return out

intr transpose<fp fty, int M, int N>(fty<M, N> inp) -> out:
    fty<N, M> out
    return out

init xavier
init zeros
init normal<float mean, float variance>

def matmul<fp fty, int M, int S, int N>(fty<M, S> t1, fty<S, N> t2):
    fty<M, N> out
    forward:
        out = matmul(t1, t2)
    backward:
        t1 = matmul(out, transpose(t2))
        t2 = matmul(transpose(t1), out)
    return out

def transpose<fp fty, int M, int N>(fty<M, N> inp) -> out:
    fty<N, M> out
    forward:
        out = transpose(inp)
    backward:
        inp = transpose(out)
    return out

def xavier<int *shape>():
    tensor<*shape> param
    return param

def linear<fp fty, int N, int M>(fty<N> inp) -> out:
    extern xavier fty<M, N> w
    extern xavier fty<M> b
    var out = reshape<M>(matmul(w, reshape<N, 1>(inp))) + b
    return add_same(reshape<M>(matmul(w, reshape<N, 1>(inp))), b)

def conv2d<int K, int M, int N, int C, int H, int W>(tensor<C, H, W> inp):
    return conv2d_intr(inp, xavier<K, C, M, N>())

def rnn_cell<int I, int S>(tensor<I> inp, tensor<S> state) -> state, out:
    var combined = concat<1>(reshape<N, -1>(inp), reshape<N, -1>(state))
    var layer1 = relu(linear(combined))
    var output = softmax(linear(layer1))
    var nstate = relu(linear(layer1))
    return nstate, output

def rnn<int I, int S>(f32<I> inp) -> out:
    extern tensor<S> state
    
    var state, var out = rnn_cell<I, S>(inp, state)
    return out

def model<int G>(f32<3, 312, 312> img) -> preds:
    var layer1 = relu(conv2d<8, 3, 3>(img))
    var layer2 = relu(conv2d<16, 3, 3>(layer1))

    tensor<16, 312, 312> tmp = layer2
    for int i in range(G):
        tmp = relu(conv2d<16, 3, 3>(tmp))
    
    var flattened = reshape<N, -1>(tmp);
    var output = rnn<N, flattened.shape[-1], 128>(flattened)

    return output

intr mul_same<fp fty, int *shape>(fty<*shape> lhs, fty<*shape> rhs) -> out:
    fty<*shape> out
    return out

def mul<fp fty, int *shape>(fty<*shape> lhs, fty<*shape> rhs) -> out:
    forward:
        var out = mul_same(lhs, rhs)
    backward:
        lhs = mul_same(rhs, out)
        rhs = mul_same(lhs, out)
    return out
