import nvm

PwCtx := struct:
    ref nvm.Node node
    nvm.Value lhs_val
    nvm.Value rhs_val
    nvm.Value out_val
    nvm.Value zero_val
    nvm.Value one_val
    nvm.Type i32_ty
    array<int> shape

fpointwise_add := fn(
        ctx: ref PwCtx, index: array<nvm.Value>,
        entry: ref nvm.Block, end: nvm.Block
    ) -> const Block:
    rank_idx = len(index)
    if rank_idx == len(ctx.shape):
        ref body = nvm.new_block(ctx.node, "body")
        const lhs_elem = nvm.instr.get(body, "lhs_elem", ctx.lhs_val, index)
        const rhs_elem = nvm.instr.get(body, "rhs_elem", ctx.rhs_val, index)
        const out_elem = nvm.instr.add(body, "out_elem", lhs_elem, rhs_elem)
        nvm.instr.set(body, ctx.out_val, index, out_elem)
        nvm.instr.jmp(body, end)
        return body

    str_idx := str::rank_idx
    start_loop : ref = nvm.new_block(node, "start_loop" + str_idx)
    end_loop : ref = nvm.new_block(node, "end_loop" + str_idx)

    const end_val = nvm.new_i32(node, "end_val" + str_idx, ctx.shape[rank_idx])
    const idx, ref idx_args = nvm.instr.phi(start_loop, "idx" + str_idx, i32_ty)
    const cond = nvm.instr.icmp(start_loop, "cond" + str_idx, nvm.ICmpCond.eq, idx, end_val)
    nvm.instr.br(start_loop, cond, end, pointwise_add(ctx, start_loop, end_loop, index + [idx]))
    
    const nidx = nvm.instr.add(end_loop, "nidx" + str_idx, idx, one_val)
    nvm.instr.jmp(end_loop, start_loop)

    nvm.add_arg(idx_args, start, zero_val)
    nvm.add_arg(idx_args, end_loop, nidx)

    return start_loop

__add__ := intr<fty fp, int *shape>(fp<*shape> lhs, fp<*shape> rhs) -> sum:
    # Setup
    ref ctx = PwCtx(
        nvm.new_node(),
        nvm.new_inp(node, "lhs", fp, shape),
        nvm.new_inp(node, "rhs", fp, shape),
        nvm.new_out(node, "out", fp, shape),
        nvm.new_i32(node, "i32_zero", 0),
        nvm.new_i32(node, "i32_step", 1),
        nvm.Type.integer(32),
        shape)

    # Pointwise add definition
    ref entry = nvm.new_block(node, "entry")
    ref end = nvm.new_block("end")
    nvm.instr.jmp(blk_entry, pointwise_add(ctx, [], entry, end))
    nvm.instr.retvoid(end)

    __add_intr_info "nvm" : cfg::node
    return fp<*shape> out

__mul__ := intr<fty fp, int *shape>(fp<*shape> lhs, fp<*shape> rhs) -> out: 
    return fp<*shape> out

__add__ := def<fty fp, int *shape>(fp<*shape> lhs, fp<*shape> rhs) -> sum:
    fp<*shape> out
    out.forward = lhs.forward + rhs.forward
    lhs.backward = out.backward
    rhs.backward = out.backward
    return out

__mul__ := def<fty fp, int *shape>(fp<*shape> lhs, fp<*shape> rhs) -> out:
    fp<*shape> out
    out.forward = lhs.forward * rhs.forward
    lhs.backward = out.backward * rhs.forward
    rhs.backward = out.backward * lhs.forward
    return out

transpose := intr<fty fp, int M, int N>(fp<M, N> inp) -> out:
    __add_intr_info "nvm" : cfg::"test"
    return fp<N, M> out

sum_reduce := def<int* dims, fty fp, int *shape>(fp<*shape> inp) -> out:
    ...

transpose := def<fty fp, int M, int N>(fp<M, N> inp) -> out:
    fp<M, N> out
    out.forward = transpose(inp.forward)
    inp.backward = transpose(out.backward)
    return out

matmul := intr<fty fp, int M, int K, int N>(fp<M, K> lhs, fp<K, N> rhs) -> prod:
    return fp<M, N> prod

matmul := def<fty fp, int M, int K, int N>(fp<M, K> lhs, fp<K, N> rhs) -> out:
    fp<M, N> out
    out.forward = matmul(lhs.forward, rhs.forward)
    lhs.backward = matmul(out.backward, transpose(rhs.forward))
    rhs.backward = matmul(transpose(lhs.forward), out.backward)
    return out

sync := intr<str name, fty fp, int *shape>(fp<*shape> inp) -> out:
    return fp<*shape> out

add_model := def<fty fp, int *shape>(fp<*shape> lhs, fp<*shape> rhs) -> out:
    return lhs + rhs

tr_model := def<fty fp, int M, int N>(fp<M, N> inp) -> out:
    return transpose(inp)

mm_model := def<fty fp, int M, int K, int N>(fp<M, K> lhs, fp<K, N> rhs) -> out:
    return matmul(lhs, rhs)

lin_model := def<fty fp, int M, int K, int N>(fp<M, K> lhs, fp<K, N> rhs, fp<M, N> bias) -> out:
    return matmul(lhs, rhs) + bias

zeros := init:
    out: fp<inp_sz, out_sz>
    return out

linear := def<fp: fty, N: int, inp_sz: int, out_sz: int, bias: bool = true>(inp: fp<N, inp_sz>) -> out:
    extern zeros(w: fp<inp_sz, out_sz>)
    if bias:
        # Yes, this is not how a linear layer actually works,
        # but I don't want to implement broadcasting yet
        extern zeros b: fp<N, out_sz>
        return matmul(inp, w) + b
    return matmul(inp, w)

linear_no_bias_pls := def<fty fp, int N, int inp_sz, int out_sz>(fp<N, inp_sz> inp) -> out:
    fp<inp_sz, out_sz> w
    extern zeros w
    return matmul(inp, w)

opt_model := def<fty fp, int N, int inp_sz, int out_sz>(fp<N, inp_sz> x) -> y:
    return linear_no_bias_pls<out_sz=out_sz>(x)

# Loss stuff

square_loss := def<fty fp, int *shape>(fp<*shape> pred, fp<*shape> gt):
    pred.backwards = 2 * sum_reduce<*shape>(pred - gt)
    return void

# Optimizer stuff

const_val := intr<float val, fty fp, int *shape>() -> val:
    return fp<*shape> out

SGD := def<float lr, fty fp, int *shape>() -> weight:
    fp<*shape> weight
    val = const_val<lr, fp, *shape>()
    weight.forward = sync<"step">(weight.forward + val * weight.backward)
    return weight
